We don't need consensus.
All agreed?
Tags: distributed state, consensus, CRDT

Bryan Boreham
Software Engineer, Weaveworks
http://weave.works
bryan@weave.works
@bboreham

* What am I talking about?
.image ./paxos.jpg
.caption "Making the world a better place through Paxos Algorithms"

* No, really!
It's about reliable systems.

- You don't want your system to go down when one component fails.
- This leads you to run multiple instances,
- So one can take over when another fails.
- So now you need to replicate your state in multiple places.
- And you need to worry about these instances getting out of sync.

_Any_system_with_more_than_one_container_is_a_distributed_system._

* Distributed state management is easy, right?

.image data-management.jpg

* Fallacies of distributed computing
- The network is reliable.
- Latency is zero.
- Bandwidth is infinite.
- The network is secure.
- Topology doesn't change.
- There is one administrator.
- Transport cost is zero.
- The network is homogeneous.

.caption - Bill Joy, Peter Deutsch et al., 20 years ago

Let's add: Hosts stay up forever.

* How can we avoid state getting out of sync?
- Only store it in one place
- Only allow updates in one place, and replicate them
- Make the nodes come to a consensus over updates
- Something else?

# At this point, I should say, if you don't want to think too hard
# about the problem, use something off-the-shelf.  Postgres, etcd,
# Consul, whatever.  It'll work pretty well, and you'll be fine.

* The cost of consensus

*Performance*: multiple network round-trips per update.

*Availability*: updates halt if you cannot contact a majority of nodes

"Network partition is indistinguishable from node failure"

.image etcd_raft_consensus.gif 300 _
.caption Image generated by [[https://github.com/ongardie/raftscope][RaftScope]]

* Why does this matter to Weaveworks?
_Connect,_observe_and_control_Docker_containers_

We have cases where data should be consistent across the network:

- Service Discovery (via DNS)
- IP address allocation

# We allocate an IP address to each container, and these addresses have
# to be unique across the whole network.

We want to support dynamically adding nodes, and we also want the
system to remain available when nodes go down or become disconnected.

We expect developers to have network hosts that come and go -
e.g. when you close your laptop and head to the coffee house.

#  And we don't want the overhead of installing and managing a store
# like Etcd or Postgres

* Examine the problem more closely

How important is it that every node is up to date?

- Service discovery: the instant after one node crashes, the list is out of date
- IP allocation: most nodes are not interested in every allocation

Can we design a data structure where it doesn't matter if some nodes
are out of date, or if updates arrive out of order?

* Solution: Convergent Replicated Data Type (CRDT)

# subset of Conflict-Free Replicated Data Type

- maintains consistency without consensus
- each local update is _monotonic_
- nodes _merge_ updates from other nodes
- merge function must be commutative, associative and idempotent

.image merge.png 320 _

* CRDT applied to DNS

Data structure is a set of entries mapping `Name` to `Address`.

- Entries also contain the host and container ID, so multiple hosts can have an entry for the same name, adding redundancy to the system. This then allows round-robin DNS for load-spreading.

- Each host only ever manipulates its own entries.

.image dns.png 300 _

* CRDT applied to IP allocation

Data structure is a ring, mapping IP address segments to hosts

- Each host only ever manipulates its own entries. When one host wants to grant some free space to another host, it simply updates one of its own entries and broadcasts an update.

.image ipam.png 320 _

* Sidebar: Broadcast and Gossip

Broadcasting every update to every node doesn't scale.

CRDT systems often use _Gossip_ mechanisms to communicate updates.

- Each node periodically tells some random selection of other nodes what it knows.
- Bandwidth usage can be controlled no matter what size your network is.

Weave adapts its gossip to work in awkward topologies,
e.g. two sets of hosts with narrow links between them.

.image topology.png

* Complications?

`Delete` is implemented by erecting a _tombstone_ over an entry
# How do we keep from accumulating an ever-increasing set of tombstoned entries?
- Tombstoned entries time out after a while

Not something you can pull off-the-shelf
 - Data structure must be carefully designed

How to bootstrap the IP allocator system?
- We use Paxos

* Three rules of thumb
- Don't force consensus unless you need to
- CRDT is a key building-block for eventual consistency.
- Take great care!

* Further reading

"Call Me Maybe" - [[https://aphyr.com/tags/jepsen][aphyr]]

CRDTs: Consistency without concurrency control
Mihai Letia, Nuno Preguiça, Marc Shapiro, 2009
[[http://pagesperso-systeme.lip6.fr/Marc.Shapiro/papers/RR-6956.pdf]]

A comprehensive study of Convergent and Commutative Replicated Data Types
Marc Shapiro, Nuno Preguiça, Carlos Baquero, Marek Zawirski, 2011
[[https://hal.inria.fr/inria-00555588/document]]
