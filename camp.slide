Distributed systems with (almost) no consensus
11 Sept 2015
Tags: distributed state, consensus, CRDT

Bryan Boreham
Software Engineer, Weaveworks
http://weave.works
bryan@weave.works
@bboreham

* What am I talking about?
.image ./paxos.jpg
.caption "Making the world a better place through Paxos Algorithms for distributed systems"

* "The server is experiencing issues"

.image data-management.jpg 520 _

* Any system with more than one container is a distributed system.

* It's about reliable systems.

- You don't want your system to go down when one component fails.
- This leads you to run multiple instances,
- So one can take over when another fails.
- So now you need to replicate your state in multiple places.
- And you need to worry about these instances getting out of sync.

_Any_system_with_more_than_one_container_is_a_distributed_system._

* Fallacies of distributed computing
- The network is reliable.
- Latency is zero.
- Bandwidth is infinite.
- The network is secure.
- Topology doesn't change.
- There is one administrator.
- Transport cost is zero.
- The network is homogeneous.

.caption - Bill Joy, Peter Deutsch et al., 20 years ago

Let's add: Hosts stay up forever.

* How can we avoid state getting out of sync?
#- Only store it in one place
#- Only allow updates in one place, and replicate them
#- Make the nodes come to a consensus over updates
#- Something else?

* Default choice: use something off-the-shelf

There are many great products for distributed state:

- Etcd, Consul, Zookeeper, ...

These all use _consensus_ algorithms like Paxos and Raft.

Consensus algorithms get very complicated, handling:

- lost messages
- delayed messages
- crashed hosts
- network partition
- adding and removing hosts

* The cost of consensus

*Availability*: halt if you cannot contact a majority of nodes

# "Network partition is indistinguishable from node failure"

*Performance*: multiple network round-trips

.image etcd_raft_consensus.gif 300 _
.caption Image generated by [[https://github.com/ongardie/raftscope][RaftScope]]

* Benchmark set-up

.image weavescope.png 500 _

.caption Image from [[http://weave.works/scope/][weave Scope]]

* Benchmark results

.image dns_qps.png 500 _

* Engineering is about picking trade-offs

* How does this matter to Weaveworks?

We have cases where data should be consistent across the network:

- Service Discovery (via DNS)
- IP address allocation

# We allocate an IP address to each container, and these addresses have
# to be unique across the whole network.

We want to support dynamically adding nodes, and we also want the
system to remain available when nodes go down or become disconnected.

We expect developers to have network hosts that come and go -
e.g. when you close your laptop and head to the coffee house.

#  And we don't want the overhead of installing and managing a store
# like Etcd or Postgres

.image weave_logo.png 60 _
.caption _Connect,_observe_and_control_Docker_containers_

* Algorithms + Data Structures

Paxos, Raft, etc., take an Algorithm-centric approach

I'm going to talk about a Data-Structure-centric approach

# (Not to discount the other way, just to give you another viewpoint)

* Examine the problem more closely

How important is it that every node is up to date?

- Service discovery: the instant after one node crashes, the list is out of date
- IP allocation: most nodes are not interested in every allocation

Can we design a data structure where it doesn't matter if some nodes
are out of date, or if updates arrive out of order?

Instead of consensus, we aim for _eventual_consistency_

* Solution: Convergent Replicated Data Type (CRDT)

# subset of Conflict-Free Replicated Data Type

- maintains consistency without consensus
- nodes _merge_ updates from other nodes
- merge function must be _commutative_, _associative_ and _idempotent_

.image merge.png 320 _

* CRDTs are Awesome

* CRDT applied to DNS

Data structure is a set of entries mapping `Name` to `Address`.

- Entries also contain the host and container ID, so multiple hosts can have an entry for the same name, adding redundancy to the system. This then allows round-robin DNS for load-spreading.

- Each host only ever manipulates its own entries.

.image dns.png 300 _

* How do we delete an entry?

`Delete` is implemented by erecting a _tombstone_ over an entry

* CRDT applied to IP allocation

Data structure is a ring, mapping IP address segments to hosts

- Each host only ever manipulates its own entries. When one host wants to grant some free space to another host, it simply updates one of its own entries and broadcasts an update.

.image ipam.png 320 _

* Sidebar: Broadcast and Gossip

Broadcasting every update to every node doesn't scale.

CRDT systems often use _Gossip_ mechanisms to communicate updates.

- Each node periodically tells some random selection of other nodes what it knows.
- Bandwidth usage can be controlled no matter what size your network is.

Weave adapts its gossip to work in awkward topologies,
e.g. two sets of hosts with narrow links between them.

.image topology.png

* Complications?

How do we keep from accumulating an ever-increasing set of tombstoned entries?
- Tombstoned entries time out after a while

CRDTs are not something you can pull off-the-shelf
 - Data structure must be carefully designed

How to bootstrap the IP allocator system?
- We use Paxos

* Three rules of thumb
- Don't force consensus unless you need to
- CRDTs are an essential part of the toolkit for eventual consistency
- Take great care when designing distributed systems!

* Questions?

.link [[http://twitter.com/@bboreham][@bboreham]]

To find out more about our products:

.link http://weave.works
.link http://github.com/weaveworks
.link help@weave.works
.link [[http://twitter.com/weaveworks][@weaveworks]]

.image weave_logo.png 60 _
.caption _Connect,_observe_and_control_Docker_containers_


* Further reading

"Call Me Maybe" - [[https://aphyr.com/tags/jepsen][aphyr]]

CRDTs: Consistency without concurrency control
Mihai Letia, Nuno Preguiça, Marc Shapiro, 2009
[[http://pagesperso-systeme.lip6.fr/Marc.Shapiro/papers/RR-6956.pdf]]

A comprehensive study of Convergent and Commutative Replicated Data Types
Marc Shapiro, Nuno Preguiça, Carlos Baquero, Marek Zawirski, 2011
[[https://hal.inria.fr/inria-00555588/document]]
